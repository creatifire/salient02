"""
Copyright (c) 2025 Ape4, Inc. All rights reserved.
Unauthorized copying of this file is strictly prohibited.
"""

from typing import Sequence
import os
import time
os.environ['USER_AGENT'] = 'myagent'
# import bs4
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
# from langchain_community.document_loaders import WebBaseLoader
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, RemoveMessage, filter_messages
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
# from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
# from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, END, StateGraph
from langgraph.graph.message import add_messages
from typing_extensions import Annotated, TypedDict, Literal
import gradio as gr
# from langchain_chroma import Chroma
from langchain_pinecone import PineconeVectorStore
import openai
import requests
import glm_prompts_03c as prompts

# Set up OpenAI API key and initialize OpenAI models
LLM_MODEL = "gpt-4o"
EMBEDDING_MODEL = "text-embedding-3-small"
openai.api_key = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model=LLM_MODEL, temperature=0)
embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL)

# Set up Pinecone API key
INDEX_NAME = "agrofresh01"
pc_api_key = os.environ.get("PINECONE_API_KEY")
vectorstore = PineconeVectorStore(
    # documents=splits,
    embedding=embedding_model,
    pinecone_api_key=pc_api_key,
    # persist_directory=DB_PATH
    index_name=INDEX_NAME
)
retriever = vectorstore.as_retriever()

### BEGIN - Defining the workflow ###

### Contextualize question ###
contextualize_q_system_prompt = prompts.CONTEXTUALIZE_SYSTEM_PROMPT
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
history_aware_retriever = create_history_aware_retriever(
    llm, retriever, contextualize_q_prompt
)


### Answer question ###
system_prompt = prompts.SYSTEM_PROMPT
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)


### Statefully manage chat history ###
class State(TypedDict):
    """
    State is a TypedDict that represents the state of a conversation.
    Attributes:
        input (str): The input message from the user.
        chat_history (Sequence[BaseMessage]): A sequence of messages representing the chat history.
        context (str): The context of the conversation.
        answer (str): The answer generated by the system.
        summary (str): A summary of the conversation.
        email (str): The email address associated with the person the chatbot is conversing with.
    Args:
        TypedDict (_type_): _description_
    """
    input: str
    chat_history: Annotated[Sequence[BaseMessage], add_messages]
    context: str
    answer: str
    summary: str
    email: str


## Sales conversation node
def converse_with_customer(state: State) -> State:
    """
    This node is responsible for handling the conversation with the customer.
    Args:
        state (State): This is the messages state that was passed to the node calling this tool
    Returns:
        State: This is the updated state with the response from the chatbot
    """
    response = rag_chain.invoke(state)
    return {
        "input": state["input"],
        "chat_history": [
            HumanMessage(state["input"]),
            AIMessage(response["answer"]),
        ] + state["chat_history"],
        "context": response["context"],
        "answer": response["answer"],
        "summary": state.get("summary", ""),
        "email": state.get("email", "")
    }

## Node to extract the email address from the most recent conversation
def extract_or_update_email(state: State) -> State:
    """
    This node function extracts an email address from the chat history if it exists.
    If no email is found, it returns an empty string.
    If the user wants to update their email address, it returns the new email address.
    If the user wants to remove their email address, it returns an empty string.
    
    Args:
        state (State): This is the messages state that was passed to the node calling this tool
        
    Returns:
        State: This is the updated state with the email address extracted or updated
    """
    chat_history = state["chat_history"]
    human_messages = filter_messages(chat_history, include_types=("human"))
    extract_email_prompt = PromptTemplate.from_template(prompts.EXTRACT_EMAIL_PROMPT)
    obtain_email_chain = extract_email_prompt | llm | StrOutputParser()
    email = obtain_email_chain.invoke({"human_messages": human_messages})
    if email:
        return {
            "input": state["input"],
            "chat_history": state["chat_history"],
            "context": state["context"],
            "answer": state["answer"],
            "summary": state.get("summary", ""),
            "email": email
        }
    return state

def should_send_an_email_summary(state: State) -> Literal["send_an_email_summary", "email_processing_completed"]:
    """
    This router function determines whether an email summary should be sent based on the chat history.
    
    Args:
        state (State): This is the messages state that was passed to the node calling this router
        
    Returns:
        Literal["send_an_email_summary", "email_processing_completed"]: This is the next
    """
    chat_history = state["chat_history"]
    human_messages = filter_messages(chat_history, include_types=("human"))
    determine_customer_intent_prompt = PromptTemplate.from_template(
        prompts.DETERMINE_WHETHER_TO_SEND_AN_EMAIL_PROMPT)
    chain = determine_customer_intent_prompt | llm | StrOutputParser()
    return chain.invoke({"human_messages": human_messages})
    

## A node that will draft and send an email
def send_an_email_summary(state: State) -> State:
    """
    This tool drafts an email and sends an email using using the mailgun API.

    Args:
        email: A string that contains the email address of the person the chatbot is conversing with.
        summary: A string that contains a summary of the conversation.
        chat_history: A list of type (Sequence[BaseMessage]) that contains conversation after the last summary was made.

    Returns:
        State: This is the updated state with the email address extracted or updated
    """
    email_to = state["email"]
    # subject = draft_email_subject_line([{"summary": state["summary"]}] + state["chat_history"])
    # email_message = draft_email_message([{"summary": state["summary"]}] + state["chat_history"])
    subject = draft_email_subject_line(state["summary"], state["chat_history"])
    email_message = draft_email_message(state["summary"], state["chat_history"])
    response = send_message_using_mailgun(email_to, subject, email_message)
    # if response.status_code == 200:
    #     return "email_sent"
    # return "email_send_failure"
    return state


def draft_email_subject_line(summary: str, chat_history: Sequence[BaseMessage]) -> str:
    draft_email_subject_line_promt = PromptTemplate.from_template(prompts.DRAFT_A_SUBJECT_LINE_PROMPT)
    subject_line = draft_email_subject_line_promt | llm | StrOutputParser()
    return subject_line.invoke({"summary": summary, "chat_history": chat_history})
    
    
def draft_email_message(summary: str, chat_history: Sequence[BaseMessage]) -> str:
    draft_email_message_promt = PromptTemplate.from_template(prompts.DRAFT_AN_EMAIL_MESSAGE_PROMPT)
    email_message = draft_email_message_promt | llm | StrOutputParser()
    return email_message.invoke({"summary": summary, "chat_history": chat_history})
    
    
def send_message_using_mailgun(email_to: str, subject: str, email_message: str) -> requests.Response:
  	return requests.post(
  		"https://api.mailgun.net/v3/mail.ape4.com/messages",
  		auth=("api", os.environ.get("MAILGUN_API_KEY")),
  		data={"from": "Excited Agrofresh Agent <arifsufi@ape4.com>",
  			"to": [email_to],
  			"subject": subject,
  			"text": email_message})


def email_processing_completed(state: State) -> State:
    return state

def should_summarize_conversation(state: State) -> Literal["summarize_conversation", END]:
    
    """Return the next node to execute."""
    
    chat_history = state["chat_history"]
    
    # If there are more than six messages, then we summarize the conversation
    if len(chat_history) > 6:
        return "summarize_conversation"
    
    # Otherwise we can just end
    return END


# Node to summarize the conversation
def summarize_conversation(state: State) -> State:
    
    # First, we get any existing summary
    summary = state.get("summary", "")

    # Create our summarization prompt 
    if summary:
        
        # A summary already exists
        summary_prompt = prompts.SUMMARIZE_IF_SUMMARY_EXISTS_PROMPT
    else:
        summary_prompt = prompts.SUMMARIZE_IF_NO_SUMMARY_EXISTS_PROMPT

    # Add prompt to our history
    chat_history_and_summary = state["chat_history"] + [HumanMessage(content=summary_prompt)]
    new_messages_summary = llm.invoke(chat_history_and_summary)
    
    # Delete all but the 2 most recent messages
    truncated_messages = [RemoveMessage(id=m.id) for m in state["chat_history"][:-2]]
    return {
        "input": state["input"],
        "chat_history": truncated_messages,
        "context": state["context"],
        "answer": state["answer"],
        "summary": new_messages_summary.content, 
        "email": state["email"]
        }

# Define the LangGraph Graph

workflow = StateGraph(state_schema=State)


# Define the nodes
workflow.add_node("converse_with_customer", converse_with_customer)
workflow.add_node("extract_or_update_email", extract_or_update_email)
# workflow.add_node("draft_and_send_email", draft_and_send_email)
workflow.add_node("send_an_email_summary", send_an_email_summary)
# workflow.add_node("tools", ToolNode(email_tools))
workflow.add_node("email_processing_completed", email_processing_completed)
workflow.add_node("summarize_conversation", summarize_conversation) 

# Connect the nodes with edges
workflow.add_edge(START, "converse_with_customer")
workflow.add_edge("converse_with_customer", "extract_or_update_email")
# workflow.add_edge("extract_or_update_email", "send_an_email_summary_if_requested")
# workflow.add_node("send_an_email_summary_if_requested", ToolNode(email_tools))
workflow.add_conditional_edges("extract_or_update_email", should_send_an_email_summary)
workflow.add_edge("send_an_email_summary", "email_processing_completed")
workflow.add_conditional_edges("email_processing_completed", should_summarize_conversation)
workflow.add_edge("summarize_conversation", END)

# Compile the workflow with memory saving
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)


### - END - Defining the workflow ###

# Set up the id which will be used to identify the thread
config = {"configurable": {"thread_id": "abc123"}}

## Customize the colors of the chat bubbles ###
custom_css = """
.message.user {
    background-color: #FDF2D1;
    border-width: 2px;
    border-color: #EEC649;
    }
.message.bot {
    background-color: #DCECD3;
    border-width: 2px;
    border-color: #3DAF2C;
    }
"""

# custom_css = """
# .message.user {
#     background-color: #F5F2A3;
#     border-width: 2px;
#     border-color: #E6E394;
#     }
# .message.bot {
#     background-color: #F5F2A3;
#     border-width: 2px;
#     border-color: #E6E394;
#     }
# .chat-window {
#     background-color: #F5F2A3;
# }
# #component-0 {
#     background-color: #F5F2A3;
# }
# button.clear-button {
#     background-color: #3DAF2C !important;
#     color: white !important;
#     border-color: #3DAF2C !important;
# }
# button[class*="clear"] {
#     background-color: #3DAF2C !important;
#     color: white !important;
#     border-color: #3DAF2C !important;
# }
# """

### Settings for displaying the logo ###
static_file_path = "static/"
gr.set_static_paths(paths=[static_file_path])
logo_file_name = "agrofresh_logo_01.png"

# with gr.Blocks(css=custom_css) as demo:
with gr.Blocks(css=custom_css) as demo:
    chatbot = gr.Chatbot(
        type="messages",
        value=[{
            "role": "assistant", 
            "content": "Hello my name is Sid, super star sales rep for AgroFresh! How may I be of assistance?"
            }],
        )
    msg = gr.Textbox()
    clear = gr.ClearButton([msg, chatbot])
    logo = gr.Image(
        value=f"{static_file_path}{logo_file_name}",
        type="filepath",
        width=100,
        height=100,
        interactive=False,
        show_download_button=False,
        show_fullscreen_button=False,
        show_label=False,
    )

    def respond(message, chat_history):
        # Add user message to chat history
        chat_history.append({"role": "user", "content": message})
        
        # SIMPLIFIED: Just use invoke for now - streaming with Gradio requires more complex setup
        try:
            result = app.invoke(
                {"input": message},
                config=config,
            )
            bot_message = result["answer"]
            chat_history.append({"role": "assistant", "content": bot_message})
            
        except Exception as e:
            print(f"Error during processing: {e}")
            error_message = f"Sorry, I encountered an error: {str(e)}"
            chat_history.append({"role": "assistant", "content": error_message})
            
        return "", chat_history

    msg.submit(respond, [msg, chatbot], [msg, chatbot])

demo.launch()