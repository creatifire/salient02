<!--
Copyright (c) 2025 Ape4, Inc. All rights reserved.
Unauthorized copying of this file is strictly prohibited.
-->

# Bug Report: Epic 0023 Directory Service

> **Last Updated**: October 29, 2025  
> **Epic**: [0023-directory-service.md](0023-directory-service.md)

## Summary

Three bugs/issues found during LLM tool calling evaluation:

1. **BUG-0023-001** (P0): SQLAlchemy concurrent operations error during parallel tool execution üî¥ **IDENTIFIED**
2. **BUG-0023-002** (P1): Configuration cascade looking in wrong path üü° **IDENTIFIED**
3. **BUG-0023-003** (P2): Connection pool exhaustion under concurrent load üü¢ **IDENTIFIED**

**Current Impact**: Parallel tool calls ‚Üí cost tracking failures ‚Üí lost billing data + abnormally short responses

**Status**: All bugs identified and documented, awaiting implementation of fixes

---

## Test Case

**Setup**: Wyckoff agent (`wyckoff/wyckoff_info_chat1`) with parallel tool calling enabled  
**Action**: LLM makes 2 parallel `search_directory` tool calls  
**Time**: 2025-10-29 15:09:12-15:09:15  
**Log Source**: `memorybank/analysis/llm-tool-calling-evaluation.md` lines 115-138

| Expected | Actual |
|----------|--------|
| Cost tracking succeeds | Cost tracking fails with "concurrent operations not permitted" |
| `llm_request_id` = UUID | `llm_request_id: None` |
| Full response saved | Abnormally short response (24 chars vs normal 4000+) |
| Billing data recorded | No cost data in database |

**Evidence**:
```python
"Cost tracking failed (non-critical): This session is provisioning a new connection; 
concurrent operations are not permitted 
(Background on this error at: https://sqlalche.me/e/20/isce)"
```

---

## BUG-0023-001: SQLAlchemy Concurrent Operations Error üî¥ P0

### Problem

When LLM makes parallel tool calls (e.g., 2x `search_directory`), cost tracking fails with SQLAlchemy concurrent operations error. This results in lost billing data and incomplete response tracking.

**Log Evidence** (2025-10-29 15:09:15):
```python
{
  "event": "cost_tracking.failed",
  "error": "This session is provisioning a new connection; concurrent operations are not permitted",
  "llm_request_id": None,  # ‚Üê Should be UUID
  "response_length": 24,   # ‚Üê Abnormally short (normal: 4000+)
  "url": "https://sqlalche.me/e/20/isce"
}
```

**Comparison - AgroFresh (Single Tool Call)**:
```python
{
  "event": "cost_tracking.success",
  "llm_request_id": "8314ca61-...",
  "total_cost": 0.00092415,
  "response_length": 4247  # ‚Üê Normal length
}
```

### Root Cause

**Shared Database Session Across Concurrent Operations**:

1. LLM makes 2 parallel `search_directory` tool calls
2. Tool execution begins database queries (session is "provisioning")
3. Cost tracking attempts to write to database concurrently
4. SQLAlchemy rejects: session already mid-transaction
5. Cost tracking fails, `llm_request_id` remains `None`

**Location**: Shared session across:
- Tool execution: `backend/app/agents/tools/directory_tools.py`
- Cost tracking: `backend/app/services/llm_request_tracker.py`

**Why Single Tool Calls Work**:
- Sequential execution: tool completes ‚Üí session released ‚Üí cost tracking begins
- No concurrent access to database session

### Impact

**Data Integrity**:
- ‚ùå Lost billing data (no cost tracking for these requests)
- ‚ùå `llm_request_id: None` prevents request tracking
- ‚ùå Audit trail gaps for parallel tool usage
- ‚ùå Cannot analyze performance of parallel vs sequential tools

**Financial**:
- ‚ùå Revenue loss (unbilled LLM usage)
- ‚ùå Cost analysis incomplete for multi-tool scenarios
- ‚ùå Cannot verify billing accuracy

**Observability**:
- ‚ùå Missing data in Logfire/analytics
- ‚ùå Cannot measure tool calling performance
- ‚ùå Incomplete usage patterns

### Fix Options

#### Option A: Sequential Tool Execution (Safest, Simplest)

**Approach**: Force tools to execute one at a time

**Implementation**:
```python
# backend/app/agents/simple_chat.py
# Modify agent configuration to disable parallel tool calling

agent = Agent(
    model=model,
    deps_type=SessionDependencies,
    system_prompt=system_prompt,
    # Add this to force sequential execution:
    result_type=None,  # Pydantic AI default
    # OR configure in agent config.yaml:
    # tool_execution_mode: "sequential"
)
```

**Pros**:
- ‚úÖ Zero code changes to database layer
- ‚úÖ Prevents all concurrent session conflicts
- ‚úÖ Simple to implement and test
- ‚úÖ No risk of data corruption

**Cons**:
- ‚ùå Slower responses (tools execute one-by-one)
- ‚ùå Defeats purpose of parallel tool calling feature
- ‚ùå Not scalable for complex multi-tool workflows

**Trade-off**: 2.4s (parallel) ‚Üí ~4.8s (sequential) for 2 tools

---

#### Option B: Session-per-Operation (Best Architecture, More Work)

**Approach**: Each operation gets its own independent database session

**Implementation**:
```python
# backend/app/services/llm_request_tracker.py

class LLMRequestTracker:
    async def track_request(
        self,
        # ... existing params ...
    ) -> UUID:
        # Create new session for this operation
        async with get_db_session() as session:
            llm_request = LLMRequest(...)
            session.add(llm_request)
            await session.commit()
            return llm_request.id

# backend/app/agents/tools/directory_tools.py

@agent.tool
async def search_directory(ctx: RunContext[SessionDependencies], ...) -> str:
    # Create new session for this tool
    async with get_db_session() as session:
        results = await directory_service.search(session, ...)
        return format_results(results)
```

**Required Changes**:
1. Update `LLMRequestTracker` to create own session (not use shared)
2. Update all `@agent.tool` functions to create own sessions
3. Update `SessionDependencies` to remove shared session
4. Add `get_db_session()` context manager for session creation
5. Update all database service methods to accept session parameter

**Pros**:
- ‚úÖ Proper async session management
- ‚úÖ No concurrent access conflicts
- ‚úÖ Scalable to complex workflows
- ‚úÖ Best practice architecture

**Cons**:
- ‚ùå Requires refactoring dependency injection
- ‚ùå Updates needed across ~10-15 files
- ‚ùå Risk of breaking existing functionality
- ‚ùå More complex testing required

**Effort**: 2-3 days of development + testing

---

#### Option C: Deferred Cost Tracking (Quick Fix, Recommended)

**Approach**: Track costs after all tools complete, not during tool execution

**Implementation**:
```python
# backend/app/agents/simple_chat.py

async def simple_chat_stream(...):
    # Store cost data in memory during streaming
    cost_tracking_data = {
        "session_id": session_id,
        "model": model,
        "tokens": {},
        "costs": {},
        # ... other tracking data
    }
    
    try:
        # 1. Execute LLM (tools may run in parallel)
        async for chunk in result.stream_text(delta=True):
            chunks.append(chunk)
            yield {"event": "message", "data": chunk}
        
        # 2. After streaming completes and tools finish
        #    NOW it's safe to write to database
        llm_request_id = await llm_request_tracker.track_request(
            **cost_tracking_data
        )
        
    except Exception as e:
        # Still track partial costs
        cost_tracking_data["completion_status"] = "error"
        await llm_request_tracker.track_request(**cost_tracking_data)
```

**Pros**:
- ‚úÖ Minimal code changes (single file)
- ‚úÖ Preserves parallel tool execution
- ‚úÖ No architecture refactoring needed
- ‚úÖ Low risk, easy to test

**Cons**:
- ‚ùå Cost tracking delayed until after response
- ‚ùå If process crashes mid-stream, costs lost
- ‚ùå Doesn't solve root cause

**Effort**: 2-4 hours of development + testing

---

### Recommendation

**Phase 1** (Immediate - This Week):
- Implement **Option C** (Deferred Cost Tracking)
- Verify with manual testing on Wyckoff agent
- Monitor for cost tracking failures in logs

**Phase 2** (Long-term - Next Quarter):
- Implement **Option B** (Session-per-Operation)
- Proper async session management architecture
- Migrate all database operations to new pattern

**Do Not Do**:
- ‚ùå **Option A** - Defeats purpose of parallel tool calling

---

### Testing

**Pre-Implementation Verification**:
```bash
# 1. Restart backend with current code
# 2. Send message to Wyckoff that triggers 2+ tool calls
# 3. Check logs for concurrent operations error
# 4. Verify llm_request_id is None in logs
```

**Expected Results** (Before Fix):
- ‚ùå Log shows: "Cost tracking failed (non-critical)"
- ‚ùå SQLAlchemy error: "concurrent operations are not permitted"
- ‚ùå `llm_request_id: None`
- ‚ùå No record in `llm_requests` table

**Post-Fix Verification** (Option C):
```bash
# 1. Apply deferred cost tracking changes
# 2. Restart backend
# 3. Send message to Wyckoff that triggers 2+ tool calls
# 4. Verify cost tracking succeeds
```

**Expected Results** (After Fix):
- ‚úÖ No "concurrent operations" errors in logs
- ‚úÖ `llm_request_id` is valid UUID
- ‚úÖ Record exists in `llm_requests` table with correct costs
- ‚úÖ All denormalized fields populated
- ‚úÖ Response length normal (~4000+ chars)

**SQL Verification**:
```sql
-- Check cost tracking for recent parallel tool requests
SELECT 
    id,
    llm_request_id,
    total_cost,
    completion_status,
    LENGTH(content) as response_length,
    created_at
FROM messages
WHERE session_id = '<test_session_id>'
ORDER BY created_at DESC;

-- Verify llm_requests table populated
SELECT 
    id,
    account_slug,
    agent_instance_slug,
    model,
    total_cost,
    prompt_tokens,
    completion_tokens,
    completion_status
FROM llm_requests
WHERE id = '<llm_request_id_from_logs>';
```

---

## BUG-0023-002: Configuration Cascade Path Resolution Error üü° P1

### Problem

Configuration cascade monitor is looking for agent config in wrong path, causing all agents to fall back to global config instead of using agent-specific settings.

**Log Evidence** (Repeats throughout both agent requests):
```python
{
  "event": "config.cascade.fallback",
  "error": "Agent configuration file not found. Tried: /Users/.../backend/config/agent_configs/simple_chat/config.yaml",
  "fallback": "Using global_config",
  "account": "wyckoff",
  "agent_instance": "wyckoff_info_chat1"
}
```

**What's Wrong**:
- Looking for: `agent_configs/simple_chat/config.yaml` (generic agent type)
- Should be: `agent_configs/wyckoff/wyckoff_info_chat1/config.yaml` (specific instance)

### Root Cause

**Incorrect Path Construction in Configuration Cascade Monitor**:

`cascade_monitor` is using `agent_type` instead of full multi-tenant instance path.

**Current Behavior**:
```python
# Incorrectly constructs path from agent_type
agent_name = "simple_chat"  # ‚Üê Wrong
config_path = f"agent_configs/{agent_name}/config.yaml"
# Result: agent_configs/simple_chat/config.yaml (doesn't exist)
```

**Correct Behavior**:
```python
# Should construct from account + instance
account_slug = "wyckoff"
instance_slug = "wyckoff_info_chat1"
config_path = f"agent_configs/{account_slug}/{instance_slug}/config.yaml"
# Result: agent_configs/wyckoff/wyckoff_info_chat1/config.yaml (exists)
```

**Location**: Likely `backend/app/agents/cascade_monitor.py` or config loader service

### Impact

**Configuration**:
- ‚ùå All agents use same model (from global config)
- ‚ùå Cannot customize temperature per agent
- ‚ùå Cannot customize max_tokens per agent
- ‚ùå Agent-specific prompts ignored
- ‚ùå Agent-specific tool configurations ignored

**Developer Experience**:
- ‚ùå Confusion: "Why aren't my config changes working?"
- ‚ùå Per-agent customization broken
- ‚ùå Multi-tenant architecture partially defeated

**Current Workaround**:
- All agents work (using global config)
- No data corruption or errors
- Just can't customize individual agents

### Fix Implementation

**Step 1: Locate Configuration Loader**

**Files to Check**:
1. `backend/app/agents/cascade_monitor.py` (if exists)
2. `backend/app/agents/simple_chat.py` - config loading section
3. `backend/app/services/config_loader.py` (if exists)
4. Any function that loads agent config and logs cascade fallback

**Step 2: Update Path Construction**

**Before**:
```python
# Current (incorrect)
def load_agent_config(agent_name: str) -> dict:
    config_path = f"backend/config/agent_configs/{agent_name}/config.yaml"
    # ...
```

**After**:
```python
# Fixed (multi-tenant path)
def load_agent_config(
    account_slug: str,
    instance_slug: str
) -> dict:
    config_path = f"backend/config/agent_configs/{account_slug}/{instance_slug}/config.yaml"
    # ...
```

**Step 3: Update Callers**

Ensure all callers pass full path parameters:
```python
# Update wherever load_agent_config is called
config = load_agent_config(
    account_slug=session.account_slug,  # From session
    instance_slug=instance_config.get("instance_name")  # From instance
)
```

### Testing

**Pre-Fix Verification**:
```bash
# Check current logs for cascade fallback
grep "config.cascade.fallback" backend/logs/*.log

# Verify agent-specific configs exist
ls -la backend/config/agent_configs/wyckoff/wyckoff_info_chat1/config.yaml
ls -la backend/config/agent_configs/agrofresh/agro_info_chat1/config.yaml
```

**Post-Fix Verification**:
```bash
# 1. Apply fix
# 2. Restart backend
# 3. Send message to Wyckoff agent
# 4. Check logs - should NOT see cascade fallback error
# 5. Verify agent uses config.yaml settings
```

**Expected Results**:
- ‚úÖ No "Agent configuration file not found" errors
- ‚úÖ Logs show: "config.cascade.success" (or no fallback message)
- ‚úÖ Agent uses model from its own `config.yaml`
- ‚úÖ Agent-specific temperature/max_tokens applied

**Config Verification**:
```yaml
# backend/config/agent_configs/wyckoff/wyckoff_info_chat1/config.yaml
# Add distinct test value
llm:
  model: "openai/gpt-4o-mini"  # Different from global
  temperature: 0.3  # Different from global (e.g., global is 0.7)
```

Then verify in logs:
```python
{
  "event": "llm.request.start",
  "model": "openai/gpt-4o-mini",  # ‚Üê Should match agent config
  "temperature": 0.3  # ‚Üê Should match agent config
}
```

### Priority

**P1** - Should fix soon:
- Blocks per-agent customization
- Defeats multi-tenant architecture intent
- Low effort, high value fix
- No data corruption risk

**Recommended Timeline**: This week (1-2 hours effort)

---

## BUG-0023-003: Connection Pool Sizing üü¢ P2

### Problem

Database connection pool may be undersized for concurrent load, potentially contributing to session conflicts.

**Current Configuration** (`backend/app/database.py`):
```python
pool_size=20          # Maximum persistent connections
max_overflow=0        # ‚Üê No overflow allowed!
pool_timeout=30       # Seconds to wait for connection
```

**Issue**: `max_overflow=0` means no burst capacity - if all 20 connections are in use, new requests wait up to 30 seconds or fail.

### Root Cause

**Conservative Pool Sizing**:
- Connection pool doesn't allow overflow
- Parallel tool execution may exhaust pool
- Forces connection reuse under concurrent load
- Can contribute to "session provisioning" conflicts

**Evidence** (Indirect):
- Concurrent operations error suggests pool pressure
- Both directory search calls execute in 2.4s (very fast)
- May indicate connection checkout conflicts

### Impact

**Performance**:
- ‚ö†Ô∏è Requests may wait for available connections
- ‚ö†Ô∏è Timeout errors possible under heavy load
- ‚ö†Ô∏è Increased latency during traffic spikes

**Stability**:
- ‚ö†Ô∏è Pool exhaustion could cause request failures
- ‚ö†Ô∏è May mask underlying session management issues

**Current Observation**:
- ‚úÖ No timeout errors observed yet
- ‚úÖ System functional under current load
- ‚ö†Ô∏è Unknown behavior under production load

### Fix Implementation

**Recommended Changes** (`backend/app/database.py`):

```python
# Before
DATABASE_URL = os.getenv("DATABASE_URL")
engine = create_async_engine(
    DATABASE_URL,
    echo=False,
    pool_size=20,
    max_overflow=0,      # ‚Üê Change this
    pool_timeout=30,
    pool_recycle=3600,
)

# After
DATABASE_URL = os.getenv("DATABASE_URL")
engine = create_async_engine(
    DATABASE_URL,
    echo=False,
    pool_size=20,        # Keep: Persistent connections
    max_overflow=10,     # NEW: Allow 10 burst connections (total max: 30)
    pool_timeout=30,     # Keep: Wait up to 30s for connection
    pool_recycle=3600,   # Keep: Recycle stale connections
    pool_pre_ping=True,  # NEW: Verify connections before use
)
```

**Changes Explained**:
1. **`max_overflow=10`**: Allow 10 temporary connections beyond pool_size
   - Total capacity: 20 (persistent) + 10 (overflow) = 30 concurrent connections
   - Overflow connections closed after use (not kept in pool)
   
2. **`pool_pre_ping=True`**: Test connection validity before checkout
   - Prevents stale connection errors
   - Small overhead, big reliability gain

**Additional Monitoring**:
```python
# Add to logging/metrics
import logfire

# Track connection pool usage
logfire.info(
    "db.pool.status",
    size=engine.pool.size(),
    checked_out=engine.pool.checkedout(),
    overflow=engine.pool.overflow(),
    checked_in=engine.pool.checkedin()
)
```

### Testing

**Pre-Fix Baseline**:
```bash
# Check current pool status (if logging exists)
grep "db.pool" backend/logs/*.log

# Load test with parallel requests
# Send 25+ concurrent requests to trigger pool limit
```

**Post-Fix Verification**:
```bash
# 1. Apply pool configuration changes
# 2. Restart backend
# 3. Send 30+ concurrent requests (exceeds base pool_size)
# 4. Verify no timeout errors
# 5. Check overflow connections created/released
```

**Expected Results**:
- ‚úÖ Base pool (20) handles normal load
- ‚úÖ Overflow connections (up to 10) handle bursts
- ‚úÖ No timeout errors under peak load
- ‚úÖ Connections recycled properly after use

**Monitoring Query** (PostgreSQL):
```sql
-- Check active connection count
SELECT count(*) as active_connections
FROM pg_stat_activity
WHERE datname = 'salient02_dev';

-- Check connection details
SELECT 
    pid,
    usename,
    application_name,
    state,
    query_start,
    state_change
FROM pg_stat_activity
WHERE datname = 'salient02_dev'
ORDER BY query_start DESC;
```

### Priority

**P2** - Nice to have:
- No errors observed with current pool settings
- May be masking BUG-0023-001 (session management)
- Should fix **after** resolving concurrent operations issue
- Could be safety net, not solution

**Recommended Timeline**: 
- After BUG-0023-001 fixed (don't mask root cause)
- Before production launch
- Combined with load testing

**Note**: Increasing pool size won't fix concurrent session conflicts - need session-per-operation architecture (BUG-0023-001 Option B).

---

## Fix Priority Order

1. **BUG-0023-001** (P0): SQLAlchemy concurrent operations ‚Üí Immediate (Option C deferred tracking)
2. **BUG-0023-002** (P1): Configuration cascade path ‚Üí This week (1-2 hours)
3. **BUG-0023-003** (P2): Connection pool sizing ‚Üí After BUG-001 fixed, before production

**Rationale**:
- Fix cost tracking first (prevents revenue loss)
- Fix config cascade next (enables per-agent customization)
- Increase pool size last (verify it's actually needed after session fix)

---

## Testing Status

**Identified**: 2025-10-29  
**Source**: `memorybank/analysis/llm-tool-calling-evaluation.md` (log analysis)

**Current Status**:
- üî¥ **BUG-001**: Confirmed via logs, awaiting fix
- üü° **BUG-002**: Confirmed via logs, awaiting fix  
- üü¢ **BUG-003**: Suspected issue, needs load testing to confirm

**Success Criteria**:
- ‚úÖ Parallel tool calls complete without cost tracking failures
- ‚úÖ `llm_request_id` is valid UUID for all requests
- ‚úÖ Agent-specific configs loaded correctly (no cascade fallback)
- ‚úÖ Connection pool handles burst traffic without timeouts

**Recommended Next Steps**:
1. Implement BUG-0023-001 Option C (deferred cost tracking)
2. Test with Wyckoff agent parallel tool calls
3. Fix BUG-0023-002 (config cascade path)
4. Verify agent-specific model settings work
5. Consider BUG-0023-003 (pool sizing) after load testing

---

**Created**: 2025-10-29 | **Updated**: 2025-10-29 | **Epic**: [0023-directory-service.md](0023-directory-service.md)

