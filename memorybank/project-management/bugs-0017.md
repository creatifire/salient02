# Bug Report: Epic 0017 Simple Chat Agent

> **Last Updated**: October 14, 2025  
> **Epic**: [0017-simple-chat-agent.md](0017-simple-chat-agent.md)

## Summary

Four bugs found testing `htmx-chat2.html` with `default_account/simple_chat2`:

1. **BUG-001** (P0): Streaming sends zero chunks but LLM completes successfully
2. **BUG-002** (P1): Model `openai/gpt-oss-120b` missing from pricing
3. **BUG-003** (P1): NULL account/agent IDs in error sessions
4. **BUG-004** (P2): Duplicate user messages on retry

**Impact**: 1 message → 2 LLM calls, 3 DB records, 1 orphaned session

---

## Test Case

**Setup**: `htmx-chat2.html` → `default_account/simple_chat2` → `openai/gpt-oss-120b`  
**Action**: Sent one message  
**Time**: 2025-10-14 22:06:33-55

| Expected | Actual |
|----------|--------|
| 1 session | 2 sessions (`56e9db39` + vapid `98dd2860`) |
| 1 LLM request | 2 requests (`ce8ffbb4` cost=0, `7826af49` cost=0.00071373) |
| 2 messages | 3 messages (user, duplicate user, assistant) |

---

## BUG-001: Zero Chunks Streaming 🔴 P0

### Problem

LLM completes (204 tokens) but streaming sends 0 chunks to client → error → POST retry.

**Log evidence**:
```json
"chunks_sent": 0,
"tokens": {"completion": 204, "total": 366},
"completion_status": "complete"
```

**Error**:
```python
ValueError: Message content cannot be empty
# Line 870: save_message(content=response_text)  # ← Empty string
```

### Root Cause

`result.stream_text()` loop not yielding chunks. Possible reasons:
1. Stream returns empty despite tokens
2. Exception before first yield
3. Model sends metadata but no text

**Location**: `backend/app/agents/simple_chat.py::simple_chat_stream()` line ~550

### Impact

- User sees 20s delay (streaming fails, POST retries)
- 2x LLM calls = 2x cost
- Duplicate DB records
- Vapid sessions created

### Fix Options

**A. Handle empty streams** (recommended):
```python
# simple_chat.py line ~550
response_text = "".join(chunks).strip()
if not response_text:
    yield {"event": "error", "data": json.dumps({"message": "No content", "retry": True})}
    return  # Don't save empty messages
```

**B. Add logging** to diagnose:
```python
chunk_count = 0
async for chunk in result.stream_text(delta=True):
    chunk_count += 1
    logger.debug({"chunk_number": chunk_count, "length": len(chunk) if chunk else 0})
    if chunk:
        chunks.append(chunk)
        yield {"event": "message", "data": chunk}
```

**C. Disable streaming** for this model:
```python
FORCE_NON_STREAMING = ["openai/gpt-oss-120b"]
```

### Tests

- Mock empty `result.stream_text()`, verify error event (not ValueError)
- Test with actual model, verify chunks > 0
- Regression test other models still work

---

## BUG-002: Missing Model Pricing 🟡 P1

### Problem

`openai/gpt-oss-120b` not in `genai-prices` or `fallback_pricing.yaml` → cost = 0.0

**Log**: `streaming_cost_calculation_failed: "Unable to find model"`  
**DB**: `prompt_cost: 0`, `completion_cost: 0`, `total_cost: 0`

**Why POST worked**: Non-streaming extracts costs from OpenRouter `provider_details`  
**Why streaming failed**: Relies on `genai-prices.calc_price()` which doesn't have this model

### Fix

Add to `backend/config/fallback_pricing.yaml`:

```yaml
openai/gpt-oss-120b:
  input_per_1m: 50.0     # Calculated: 0.00000505/101 tokens * 1M
  output_per_1m: 280.0   # Calculated: 0.00070868/2531 tokens * 1M
  source: "OpenRouter provider_details"
  updated: "2025-10-14"
```

Restart backend, test, verify costs > 0 in DB.

---

## BUG-003: Vapid Sessions (NULL IDs) 🟡 P1

### Problem

POST retry creates orphaned session with NULL `account_id`, `account_slug`, `agent_instance_id`.

**Timeline**:
- 22:06:35.232 - Streaming fails
- 22:06:35.235 - Vapid session `98dd2860` created
- 22:06:35.262 - POST retry (uses vapid session)

### Root Cause

`chat_endpoint()` missing session context update that `stream_endpoint()` and `history_endpoint()` already do.

**Location**: `backend/app/api/account_agents.py::chat_endpoint()` line ~430

### Fix

Add session context update in `chat_endpoint()`:

```python
@router.post("/accounts/{account_slug}/agents/{instance_slug}/chat")
async def chat_endpoint(...):
    # Add this before calling simple_chat():
    await session_service.update_session_context(
        session_id=request.state.session_id,
        account_id=account.id,
        account_slug=account.slug,
        agent_instance_id=instance.id
    )
```

Verify query returns 0 rows:
```sql
SELECT * FROM sessions WHERE account_id IS NULL;
```

---

## BUG-004: Duplicate User Messages 🟢 P2

### Problem

POST retry saves user message again (already saved during streaming).

**Result**: 2 user messages + 1 assistant message in DB (expected: 1 + 1)

### Root Cause

**By design**: Both paths save user message to prevent data loss. POST doesn't check if already exists.

### Recommendation

**Won't fix** because:
- Streaming will be fixed (BUG-001) → retries become rare
- Prevents message loss during errors
- Easy to filter duplicates in queries
- System more robust with current approach

**Optional fix** (if UI/UX issue):
- Add idempotency check in `message_service.save_message()`
- Check for same session + role + content + recent timestamp
- Or add request_id to deduplicate

---

## Fix Order

1. **BUG-001** (P0): Breaks streaming → cascading failures
2. **BUG-002** (P1): Quick config fix, prevents cost tracking errors
3. **BUG-003** (P1): Data integrity issue, fix after BUG-001 to reduce test noise
4. **BUG-004** (P2): Won't fix unless UI/UX requires it

---

## Testing

**Verify after fixes**:
- Test `htmx-chat2.html` → should create 1 session, 1 LLM request, 2 messages
- Check DB: costs > 0, no NULL account_id, no duplicates
- Regression: test all other agents still work

**Success criteria**:
- Streaming works for `openai/gpt-oss-120b`
- POST retry only on genuine network errors (not streaming bugs)
- Zero vapid sessions
- Accurate cost tracking

---

**Created**: 2025-10-14 | **Epic**: [0017-simple-chat-agent.md](0017-simple-chat-agent.md)

