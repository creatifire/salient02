# Bug Report: Epic 0017 Simple Chat Agent

> **Last Updated**: October 14, 2025  
> **Epic**: [0017-simple-chat-agent.md](0017-simple-chat-agent.md)

## Summary

Four bugs found testing `htmx-chat2.html` with `default_account/simple_chat2`:

1. **BUG-0017-001** (P0): Streaming sends zero chunks but LLM completes successfully ✅ **FIXED**
2. **BUG-0017-002** (P1): Model `openai/gpt-oss-120b` missing from pricing
3. **BUG-0017-003** (P1): NULL account/agent IDs in error sessions
4. **BUG-0017-004** (P2): Duplicate user messages on retry

**Original Impact**: 1 message → 2 LLM calls, 3 DB records, 1 orphaned session  
**After BUG-0017-001 Fix**: 1 message → 1 LLM call, streaming works correctly

---

## Test Case

**Setup**: `htmx-chat2.html` → `default_account/simple_chat2` → `openai/gpt-oss-120b`  
**Action**: Sent one message  
**Time**: 2025-10-14 22:06:33-55

| Expected | Actual |
|----------|--------|
| 1 session | 2 sessions (`56e9db39` + vapid `98dd2860`) |
| 1 LLM request | 2 requests (`ce8ffbb4` cost=0, `7826af49` cost=0.00071373) |
| 2 messages | 3 messages (user, duplicate user, assistant) |

---

## BUG-0017-001: Zero Chunks Streaming ✅ FIXED

### Problem

LLM completes (204 tokens) but streaming sends 0 chunks to client → error → POST retry.

**Log evidence**:
```json
"chunks_sent": 0,
"tokens": {"completion": 204, "total": 366},
"completion_status": "complete"
```

**Error**:
```python
ValueError: Message content cannot be empty
# Line 870: save_message(content=response_text)  # ← Empty string
```

### Root Cause

`result.stream_text()` loop not yielding chunks. Possible reasons:
1. Stream returns empty despite tokens
2. Exception before first yield
3. Model sends metadata but no text

**Location**: `backend/app/agents/simple_chat.py::simple_chat_stream()` line ~550

### Impact

- User sees 20s delay (streaming fails, POST retries)
- 2x LLM calls = 2x cost
- Duplicate DB records
- Vapid sessions created

### Fix Options

**A. Handle empty streams** (recommended):
```python
# simple_chat.py line ~550
response_text = "".join(chunks).strip()
if not response_text:
    yield {"event": "error", "data": json.dumps({"message": "No content", "retry": True})}
    return  # Don't save empty messages
```

**B. Add logging** to diagnose root cause:
```python
chunk_count = 0
async for chunk in result.stream_text(delta=True):
    chunk_count += 1
    logger.debug({"chunk_number": chunk_count, "length": len(chunk) if chunk else 0})
    if chunk:
        chunks.append(chunk)
        yield {"event": "message", "data": chunk}
```

### Fix Applied

**Date**: 2025-10-15  
**Commit**: `273da61` - "fix(streaming): handle empty LLM responses gracefully"

**Changes**:
1. Added empty stream handling (Option A)
   - Checks if `response_text` is empty after streaming completes
   - Yields error event to client instead of attempting to save
   - Returns early to prevent `ValueError`
2. Added diagnostic logging (Option B)
   - Tracks `chunk_count` during streaming loop
   - Logs each chunk received with debug level
   - Logs completion summary with chunks sent

**Verified**:
- ✅ Streaming works for `openai/gpt-oss-120b` (11 chunks sent)
- ✅ No `ValueError: Message content cannot be empty`
- ✅ No POST retry fallback
- ✅ Prevents cascading failures (BUG-0017-003, BUG-0017-004)

**Test record**: `llm_requests.id = b2ae58c0-20ca-495a-bae1-93ef7aa5edaa`

---

## BUG-0017-002: Missing Model Pricing 🟡 P1

### Problem

`openai/gpt-oss-120b` not in `genai-prices` or `fallback_pricing.yaml` → cost = 0.0

**Log**: `streaming_cost_calculation_failed: "Unable to find model"`  
**DB**: `prompt_cost: 0`, `completion_cost: 0`, `total_cost: 0`

**Why POST worked**: Non-streaming extracts costs from OpenRouter `provider_details`  
**Why streaming failed**: Relies on `genai-prices.calc_price()` which doesn't have this model

### Fix

Add to `backend/config/fallback_pricing.yaml`:

```yaml
openai/gpt-oss-120b:
  input_per_1m: 0.04     # $0.04 per 1M input tokens
  output_per_1m: 0.40    # $0.40 per 1M output tokens
  source: "https://openrouter.ai/models?q=gpt-oss-120b"
  updated: "2025-10-14"
```

Restart backend, test, verify costs > 0 in DB.

---

## BUG-0017-003: Vapid Sessions (NULL IDs) 🟡 P1

### Problem

POST retry creates orphaned session with NULL `account_id`, `account_slug`, `agent_instance_id`.

**Timeline**:
- 22:06:35.232 - Streaming fails
- 22:06:35.235 - Vapid session `98dd2860` created
- 22:06:35.262 - POST retry (uses vapid session)

### Root Cause

`chat_endpoint()` missing session context update that `stream_endpoint()` and `history_endpoint()` already do.

**Location**: `backend/app/api/account_agents.py::chat_endpoint()` line ~430

### Fix

Add session context update in `chat_endpoint()`:

```python
@router.post("/accounts/{account_slug}/agents/{instance_slug}/chat")
async def chat_endpoint(...):
    # Add this before calling simple_chat():
    await session_service.update_session_context(
        session_id=request.state.session_id,
        account_id=account.id,
        account_slug=account.slug,
        agent_instance_id=instance.id
    )
```

Verify query returns 0 rows:
```sql
SELECT * FROM sessions WHERE account_id IS NULL;
```

---

## BUG-0017-004: Duplicate User Messages 🟢 P2

### Problem

POST retry saves user message again (already saved during streaming).

**Result**: 2 user messages + 1 assistant message in DB (expected: 1 + 1)

### Root Cause

**By design**: Both paths save user message to prevent data loss. POST doesn't check if already exists.

### Recommendation

**Won't fix** because:
- Streaming will be fixed (BUG-001) → retries become rare
- Prevents message loss during errors
- Easy to filter duplicates in queries
- System more robust with current approach

**Optional fix** (if UI/UX issue):
- Add idempotency check in `message_service.save_message()`
- Check for same session + role + content + recent timestamp
- Or add request_id to deduplicate

---

## Fix Order

1. **BUG-0017-001** (P0): Breaks streaming → cascading failures ✅ **FIXED 2025-10-15**
2. **BUG-0017-002** (P1): Quick config fix, prevents cost tracking errors ⏳ **NEXT**
3. **BUG-0017-003** (P1): Data integrity issue, fix after BUG-0017-001 to reduce test noise
4. **BUG-0017-004** (P2): Won't fix unless UI/UX requires it

---

## Testing

**Verify after fixes**:
- Test `htmx-chat2.html` → should create 1 session, 1 LLM request, 2 messages
- Check DB: costs > 0, no NULL account_id, no duplicates
- Regression: test all other agents still work

**Success criteria**:
- Streaming works for `openai/gpt-oss-120b`
- POST retry only on genuine network errors (not streaming bugs)
- Zero vapid sessions
- Accurate cost tracking

---

**Created**: 2025-10-14 | **Epic**: [0017-simple-chat-agent.md](0017-simple-chat-agent.md)

