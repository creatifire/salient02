# Bug Report: Epic 0017 Simple Chat Agent

> **Last Updated**: October 15, 2025  
> **Epic**: [0017-simple-chat-agent.md](0017-simple-chat-agent.md)

## Summary

Five bugs found during testing:

1. **BUG-0017-001** (P0): Streaming sends zero chunks but LLM completes successfully ✅ **FIXED**
2. **BUG-0017-002** (P1): Model `openai/gpt-oss-120b` missing from pricing ✅ **FIXED**
3. **BUG-0017-003** (P1): NULL account/agent IDs in error sessions
4. **BUG-0017-004** (P2): Duplicate user messages on retry
5. **BUG-0017-005** (P1): Missing denormalized fields in llm_requests table

**Original Impact**: 1 message → 2 LLM calls, 3 DB records, 1 orphaned session  
**After Fixes**: Streaming works, costs tracked accurately (requires backend restart)

---

## Test Case

**Setup**: `htmx-chat2.html` → `default_account/simple_chat2` → `openai/gpt-oss-120b`  
**Action**: Sent one message  
**Time**: 2025-10-14 22:06:33-55

| Expected | Actual |
|----------|--------|
| 1 session | 2 sessions (`56e9db39` + vapid `98dd2860`) |
| 1 LLM request | 2 requests (`ce8ffbb4` cost=0, `7826af49` cost=0.00071373) |
| 2 messages | 3 messages (user, duplicate user, assistant) |

---

## BUG-0017-001: Zero Chunks Streaming ✅ FIXED

### Problem

LLM completes (204 tokens) but streaming sends 0 chunks to client → error → POST retry.

**Log evidence**:
```json
"chunks_sent": 0,
"tokens": {"completion": 204, "total": 366},
"completion_status": "complete"
```

**Error**:
```python
ValueError: Message content cannot be empty
# Line 870: save_message(content=response_text)  # ← Empty string
```

### Root Cause

`result.stream_text()` loop not yielding chunks. Possible reasons:
1. Stream returns empty despite tokens
2. Exception before first yield
3. Model sends metadata but no text

**Location**: `backend/app/agents/simple_chat.py::simple_chat_stream()` line ~550

### Impact

- User sees 20s delay (streaming fails, POST retries)
- 2x LLM calls = 2x cost
- Duplicate DB records
- Vapid sessions created

### Fix Options

**A. Handle empty streams** (recommended):
```python
# simple_chat.py line ~550
response_text = "".join(chunks).strip()
if not response_text:
    yield {"event": "error", "data": json.dumps({"message": "No content", "retry": True})}
    return  # Don't save empty messages
```

**B. Add logging** to diagnose root cause:
```python
chunk_count = 0
async for chunk in result.stream_text(delta=True):
    chunk_count += 1
    logger.debug({"chunk_number": chunk_count, "length": len(chunk) if chunk else 0})
    if chunk:
        chunks.append(chunk)
        yield {"event": "message", "data": chunk}
```

### Fix Applied

**Date**: 2025-10-15  
**Commit**: `273da61` - "fix(streaming): handle empty LLM responses gracefully"

**Changes**:
1. Added empty stream handling (Option A)
   - Checks if `response_text` is empty after streaming completes
   - Yields error event to client instead of attempting to save
   - Returns early to prevent `ValueError`
2. Added diagnostic logging (Option B)
   - Tracks `chunk_count` during streaming loop
   - Logs each chunk received with debug level
   - Logs completion summary with chunks sent

**Verified**:
- ✅ Streaming works for `openai/gpt-oss-120b` (11 chunks sent)
- ✅ No `ValueError: Message content cannot be empty`
- ✅ No POST retry fallback
- ✅ Prevents cascading failures (BUG-0017-003, BUG-0017-004)

**Test record**: `llm_requests.id = b2ae58c0-20ca-495a-bae1-93ef7aa5edaa`

---

## BUG-0017-002: Missing Model Pricing ✅ FIXED

### Problem

`openai/gpt-oss-120b` not in `genai-prices` or `fallback_pricing.yaml` → cost = 0.0

**Log**: `streaming_cost_calculation_failed: "Unable to find model"`  
**DB**: `prompt_cost: 0`, `completion_cost: 0`, `total_cost: 0`

**Why POST worked**: Non-streaming extracts costs from OpenRouter `provider_details`  
**Why streaming failed**: Relies on `genai-prices.calc_price()` which doesn't have this model

### Fix

Add to `backend/config/fallback_pricing.yaml`:

```yaml
openai/gpt-oss-120b:
  input_per_1m: 0.04     # $0.04 per 1M input tokens
  output_per_1m: 0.40    # $0.40 per 1M output tokens
  source: "https://openrouter.ai/models?q=gpt-oss-120b"
  updated: "2025-10-14"
```

### Fix Applied

**Date**: 2025-10-15  
**Commit**: `9f4043f` - "fix(pricing): update Moonshot AI model pricing to official OpenRouter rates"

**Changes**:
1. Added `openai/gpt-oss-120b` to fallback pricing config
   - input_per_1m: 0.04 ($0.04 per 1M tokens)
   - output_per_1m: 0.40 ($0.40 per 1M tokens)
   - source: https://openrouter.ai/models?q=gpt-oss-120b
2. Updated documentation with OpenRouter URL pattern guidance
3. Fixed incorrect pricing for `moonshotai/kimi-k2-0905` (was 0.14/2.49, now 0.39/1.90)
4. Updated all source URLs to official OpenRouter pages

**Verified**:
- ✅ Pricing config now includes gpt-oss-120b
- ✅ All models sourced from official OpenRouter pages
- ✅ Restart backend required for changes to take effect

**Testing Required**:
- Restart Python backend
- Send streaming request to simple_chat2
- Verify `llm_requests` table shows non-zero costs
- Expected: prompt_cost ≈ $0.0001, completion_cost ≈ $0.00005

---

## BUG-0017-003: Vapid Sessions (NULL IDs) 🟡 P1

### Problem

POST retry creates orphaned session with NULL `account_id`, `account_slug`, `agent_instance_id`.

**Timeline**:
- 22:06:35.232 - Streaming fails
- 22:06:35.235 - Vapid session `98dd2860` created
- 22:06:35.262 - POST retry (uses vapid session)

### Root Cause

`chat_endpoint()` missing session context update that `stream_endpoint()` and `history_endpoint()` already do.

**Location**: `backend/app/api/account_agents.py::chat_endpoint()` line ~430

### Fix

Add session context update in `chat_endpoint()`:

```python
@router.post("/accounts/{account_slug}/agents/{instance_slug}/chat")
async def chat_endpoint(...):
    # Add this before calling simple_chat():
    await session_service.update_session_context(
        session_id=request.state.session_id,
        account_id=account.id,
        account_slug=account.slug,
        agent_instance_id=instance.id
    )
```

Verify query returns 0 rows:
```sql
SELECT * FROM sessions WHERE account_id IS NULL;
```

---

## BUG-0017-004: Duplicate User Messages 🟢 P2

### Problem

POST retry saves user message again (already saved during streaming).

**Result**: 2 user messages + 1 assistant message in DB (expected: 1 + 1)

### Root Cause

**By design**: Both paths save user message to prevent data loss. POST doesn't check if already exists.

### Recommendation

**Won't fix** because:
- Streaming will be fixed (BUG-001) → retries become rare
- Prevents message loss during errors
- Easy to filter duplicates in queries
- System more robust with current approach

**Optional fix** (if UI/UX issue):
- Add idempotency check in `message_service.save_message()`
- Check for same session + role + content + recent timestamp
- Or add request_id to deduplicate

---

## BUG-0017-005: Missing Denormalized Fields in LLM Requests 🟡 P1

### Problem

Denormalized columns in `llm_requests` table are NULL despite schema and indexes existing:
- `account_id`: NULL (should be UUID from session)
- `account_slug`: NULL (should be 'agrofresh', 'wyckoff', etc.)
- `agent_instance_slug`: NULL (should be 'agro_info_chat1', etc.)
- `agent_type`: NULL (should be 'simple_chat')
- `completion_status`: NULL (should be 'complete', 'partial', 'error')

**DB Evidence**:
```sql
-- All recent records show NULL for denormalized fields
SELECT account_id, account_slug, agent_instance_slug, agent_type, completion_status
FROM llm_requests 
ORDER BY created_at DESC LIMIT 5;

-- Result: All NULL despite agent_instance_id being populated
```

**Verified**: 2025-10-15 - 10 most recent records all have NULL values

### Root Cause

`LLMRequestTracker.track_request()` not populating denormalized fields when saving.

**Location**: `backend/app/services/llm_request_tracker.py`

These fields were added to schema (Epic 0022-001-005) for fast billing aggregation queries without JOINs, but tracker service was never updated to populate them.

### Impact

- ❌ Billing queries require JOINs across 3 tables (slow for large datasets)
- ❌ Cannot filter by account_slug or agent_type without JOINs
- ❌ Indexes on these columns are unused
- ❌ Analytics dashboards will be slow
- ❌ Cost reports by account require complex queries

**Current workaround**: Must JOIN to get account/agent info:
```sql
SELECT lr.*, a.slug AS account, ai.instance_slug, ai.agent_type
FROM llm_requests lr
JOIN sessions s ON lr.session_id = s.id
JOIN agent_instances ai ON s.agent_instance_id = ai.id
JOIN accounts a ON ai.account_id = a.id;
```

### Fix Implementation

**Step 1: Update `llm_request_tracker.py`** - Add required parameters:

```python
# backend/app/services/llm_request_tracker.py
async def track_request(
    self,
    session_id: UUID,
    provider: str,
    model: str,
    request_body: dict,
    response_body: dict,
    tokens: dict,
    cost_data: dict,
    latency_ms: int,
    agent_instance_id: UUID,  # Already required
    account_id: UUID,  # NEW - required
    account_slug: str,  # NEW - required
    agent_instance_slug: str,  # NEW - required
    agent_type: str,  # NEW - required
    completion_status: str = "complete"  # NEW - default "complete"
) -> UUID:
    llm_request = LLMRequest(
        id=uuid4(),
        session_id=session_id,
        provider=provider,
        model=model,
        request_body=request_body,
        response_body=response_body,
        prompt_tokens=tokens.get("prompt", 0),
        completion_tokens=tokens.get("completion", 0),
        total_tokens=tokens.get("total", 0),
        prompt_cost=cost_data.get("prompt_cost", 0.0),
        completion_cost=cost_data.get("completion_cost", 0.0),
        total_cost=cost_data.get("total_cost", 0.0),
        latency_ms=latency_ms,
        agent_instance_id=agent_instance_id,
        account_id=account_id,  # NEW
        account_slug=account_slug,  # NEW
        agent_instance_slug=agent_instance_slug,  # NEW
        agent_type=agent_type,  # NEW
        completion_status=completion_status,  # NEW
        created_at=datetime.now(UTC)
    )
```

**Step 2: Update `simple_chat_stream()` in `simple_chat.py`**:

```python
# Line ~800 in simple_chat.py
# Extract denormalized fields from session and config
account_id = session.account_id  # From loaded session
account_slug = session.account_slug  # From loaded session
agent_instance_slug = instance_config.get("instance_name")  # From config
agent_type = instance_config.get("agent_type", "simple_chat")  # From config

llm_request_id = await llm_request_tracker.track_request(
    session_id=UUID(session_id),
    provider="openrouter",
    model=tracking_model,
    request_body={...},
    response_body=response_body_full,
    tokens={...},
    cost_data=cost_data,
    latency_ms=latency_ms,
    agent_instance_id=agent_instance_id,
    account_id=account_id,  # NEW
    account_slug=account_slug,  # NEW
    agent_instance_slug=agent_instance_slug,  # NEW
    agent_type=agent_type,  # NEW
    completion_status="complete"  # or "partial" in error handler
)
```

**Step 3: Update `simple_chat()` in `simple_chat.py`** (same pattern for non-streaming):

```python
# Similar changes at line ~400 in simple_chat.py
account_id = session.account_id
account_slug = session.account_slug
agent_instance_slug = instance_config.get("instance_name")
agent_type = instance_config.get("agent_type", "simple_chat")

llm_request_id = await llm_request_tracker.track_request(
    # ... all params including new denormalized fields
)
```

**Step 4: Update `LLMRequest` model if needed** - Verify `to_dict()` includes new fields:

```python
# backend/app/models/llm_request.py - verify these are in to_dict()
def to_dict(self) -> dict:
    return {
        # ... existing fields ...
        "account_id": str(self.account_id) if self.account_id else None,
        "account_slug": self.account_slug,
        "agent_instance_slug": self.agent_instance_slug,
        "agent_type": self.agent_type,
        "completion_status": self.completion_status,
    }
```

### Data Sources

| Field | Source | Location |
|-------|--------|----------|
| `account_id` | Session object | `session.account_id` (already loaded) |
| `account_slug` | Session object | `session.account_slug` (already loaded) |
| `agent_instance_slug` | Config | `instance_config.get("instance_name")` |
| `agent_type` | Config | `instance_config.get("agent_type", "simple_chat")` |
| `completion_status` | Hardcoded | `"complete"` / `"partial"` / `"error"` |

**Note**: All sources are already loaded by caller - no extra database queries needed.

### completion_status Values

Documented string values (not enforced enum):
- `"complete"` - Normal successful completion
- `"partial"` - Streaming interrupted, partial response saved
- `"error"` - LLM request failed
- `"timeout"` - Request timed out

### Testing

**Pre-implementation**:
```sql
-- Delete existing test data (no backward compatibility needed)
DELETE FROM llm_requests;
```

**Post-implementation verification**:
```sql
-- Verify all new records have denormalized fields populated
SELECT 
    id,
    account_id,
    account_slug,
    agent_instance_slug,
    agent_type,
    completion_status,
    model,
    total_cost
FROM llm_requests 
ORDER BY created_at DESC 
LIMIT 10;
```

**Expected results**:
- ✅ All fields non-NULL
- ✅ `account_slug` matches agent URL (e.g., 'agrofresh', 'wyckoff')
- ✅ `agent_instance_slug` matches config (e.g., 'agro_info_chat1')
- ✅ `agent_type` = 'simple_chat' for all agents
- ✅ `completion_status` = 'complete' for successful requests

**Fast billing query** (now works without JOINs):
```sql
-- Group costs by account - fast with indexes
SELECT 
    account_slug,
    COUNT(*) AS requests,
    SUM(total_cost) AS total_cost,
    AVG(latency_ms) AS avg_latency_ms
FROM llm_requests
WHERE created_at >= NOW() - INTERVAL '7 days'
GROUP BY account_slug
ORDER BY total_cost DESC;
```

---

## Fix Order

1. **BUG-0017-001** (P0): Breaks streaming → cascading failures ✅ **FIXED 2025-10-15**
2. **BUG-0017-002** (P1): Quick config fix, prevents cost tracking errors ✅ **FIXED 2025-10-15**
3. **BUG-0017-003** (P1): Data integrity issue (vapid sessions)
4. **BUG-0017-005** (P1): Missing denormalized fields (billing queries) ⏳ **QUESTIONS**
5. **BUG-0017-004** (P2): Won't fix unless UI/UX requires it

---

## Testing

**Verify after fixes**:
- Test `htmx-chat2.html` → should create 1 session, 1 LLM request, 2 messages
- Check DB: costs > 0, no NULL account_id, no duplicates
- Regression: test all other agents still work

**Success criteria**:
- Streaming works for `openai/gpt-oss-120b`
- POST retry only on genuine network errors (not streaming bugs)
- Zero vapid sessions
- Accurate cost tracking

---

**Created**: 2025-10-14 | **Epic**: [0017-simple-chat-agent.md](0017-simple-chat-agent.md)

